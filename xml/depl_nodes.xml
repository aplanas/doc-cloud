<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
  </dm:docmanager>
 </info>
 <para>
  After the nodes are installed and configured you can start deploying the
  &ostack; components to finalize the installation. The components need to be
  deployed in a given order, because they depend on one another. The
  <guimenu>Pacemaker</guimenu> component for an &hasetup; is the only
  exception from this rule&mdash;it can be set up at any time. However,
  when deploying &productname; from scratch, it is recommended to deploy
  the <guimenu>Pacemaker</guimenu> proposal(s) first. Deployment for all
  components is done from the &crow; Web interface through recipes,
  so-called <quote>&barcl;s</quote>.
 </para>
 <para>
  The components controlling the cloud (including storage management and
  control components) need to be installed on the &contrnode;(s) (refer to
  <xref linkend="sec.depl.arch.components.control"/> for more information).
  However, you may <emphasis>not</emphasis> use your &contrnode;(s) as a
  compute node or storage host for &o_objstore; or &ceph;. Here is a
  list with components that may <emphasis>not</emphasis> be installed on the
  &contrnode;(s): <guimenu>swift-storage</guimenu>, all &ceph;
  components, <guimenu>nova-compute-*</guimenu>. These components need to be
  installed on dedicated nodes.
 </para>
 <para>
  When deploying an &hasetup;, the controller nodes are replaced by one or
  more controller clusters consisting of at least two nodes (three are
  recommended). Setting up three separate clusters&mdash;for data,
  services, and networking&mdash;is recommended. See
  <xref linkend="sec.depl.req.ha"/> for more information on requirements and
  recommendations for an &hasetup;.
 </para>
 <para>
  The &ostack; components need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl;, refer to
  <xref linkend="sec.depl.ostack.barclamps"/>. Deploying Pacemaker (only
  needed for an &hasetup;), &o_objstore; and &ceph; is optional;
  all other components must be deployed.
 </para>
 <orderedlist spacing="normal">
  <listitem>
<!-- Pacemaker -->
   <para>
    <xref linkend="sec.depl.ostack.pacemaker" xrefstyle="select:title
    nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- RabbitMQ -->
   <para>
    <xref linkend="sec.depl.ostack.rabbit" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Ceph -->
   <para>
    <xref linkend="sec.depl.ostack.ceph" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Cinder -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.cinder" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Neutrum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.quantum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Horizon -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Heat -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.heat" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Ceilometer -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.ceilometer" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Manila -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.manila" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Trove -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.trove" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Tempest -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.tempest" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <!-- Magnum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.magnum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
 </orderedlist>

 <xi:include href="depl_pacemaker_i.xml"/>
 <xi:include href="depl_db_i.xml"/>
 <xi:include href="depl_rabbit_i.xml"/>
 <xi:include href="depl_keystone_i.xml"/>
 <xi:include href="depl_ceph_i.xml"/>
 <xi:include href="depl_swift_i.xml"/>
 <xi:include href="depl_glance_i.xml"/>
 <xi:include href="depl_cinder_i.xml"/>
 <xi:include href="depl_quantum_i.xml"/>
 <xi:include href="depl_nova_i.xml"/>

 <sect1 xml:id="sec.depl.ostack.dash">
  <title>Deploying &o_dash; (&ostack; &dash;)</title>

  <para>
   The last component that needs to be deployed is &o_dash;, the
   &ostack; &dash;. It provides a Web interface for users to start and
   stop &vmguest;s and for administrators to manage users, groups, roles,
   etc. &o_dash; should be installed on a &contrnode;. To make
   &o_dash; highly available, deploy it on a cluster.
  </para>

  <para>
   The following attributes can be configured:
  </para>

  <variablelist>
   <varlistentry>
    <term>Session Timeout</term>
    <listitem>
     <para>
      Timeout (in minutes) after which a user is been logged out automatically.
      The default value is set to four hours (240 minutes).
     </para>

     <note>
      <title>Timeouts Larger than Four Hours</title>
      <para>
       Every &o_dash; session requires a valid &o_ident; token. These tokens
       also have a lifetime of for hours (14400 seconds). Setting the &o_dash;
       session timeout to a value larger than 240 will therefore have no
       effect, and you will receive a warning when applying the &barcl;.
      </para>
      <para>
       To successfully apply a timeout larger than four hours, you first need
       to adjust the &o_ident; token expiration accordingly. To do so, open the
       &o_ident; &barcl; in <guimenu>Raw</guimenu> mode and adjust the value of
       the key <literal>token_expiration</literal>. Note that the value has to
       be provided in <emphasis>seconds</emphasis>. When the change is
       successfully applied, you can adjust the &o_dash; session timeout (in
       <emphasis>minutes</emphasis>). Note that extending the &o_ident; token
       expiration may cause scalability issues in large and very busy &cloud;
       installations.
      </para>
     </note>

    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      User Password Validation: Regular expression used for password
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Specify a regular expression with which to check the password. The
      default expression (<literal>.{8,}</literal>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see
      <link xlink:href="http://docs.python.org/2.7/library/re.html#module-re"/>
      for a reference).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      User Password Validation: Text to display if the password does not pass
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Error message that will be displayed in case the password validation
      fails.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>, you have two choices. You can either
      <guimenu>Generate (self-signed) certificates</guimenu> or provide the
      locations for the certificate key pair files
      and,&mdash;optionally&mdash; the certificate chain file. Using
      self-signed certificates is for testing purposes only and should never
      be used in production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dash; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.dash.ha">
   <title>&haSetup; for &o_dash;</title>
   <para>
    Making &o_dash; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.heat">
  <title>Deploying &o_orch; (Optional)</title>

  <para>
   &o_orch; is a template-based orchestration engine that enables you to, for
   example, start workloads requiring multiple servers or to automatically
   restart &vmguest;s if needed. It also brings auto-scaling to &cloud; by
   automatically starting additional &vmguest;s if certain criteria are
   met. For more information about &o_orch; refer to the &ostack; documentation
   at <link xlink:href="http://docs.openstack.org/developer/heat/"/>.
  </para>

  <para>
   &o_orch; should be deployed on a &contrnode;. To make &o_orch;
   highly available, deploy it on a cluster.
  </para>

  <para>
   The following attributes can be configured for &o_orch;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If
      choosing <guimenu>HTTPS</guimenu>,refer to
      <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration
      details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_orch; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_heat.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_heat.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.heat.delegated_roles">
   <title>Enabling Identity Trusts Authorization (Optional)</title>
   <para>
    Heat uses Keystone Trusts to delegate a subset of user roles to the
    &o_orch; engine for deferred operations (see <link
    xlink:href="http://hardysteven.blogspot.de/2014/04/heat-auth-model-updates-part-1-trusts.html">Steve
    Hardy's blog</link> for details ). It can either delegate all user
    roles or only those specified in the
    <literal>trusts_delegated_roles</literal> setting. Consequently, all roles
    listed in <literal>trusts_delegated_roles</literal> need to be assigned to
    a user, otherwise the user will not be able to use &o_orch;.
   </para>
   <para>
    The recommended setting for <literal>trusts_delegated_roles</literal> is
    <literal>Member</literal>, since this is the default role most users are
    likely to have. This is also the default setting when installing &cloud;
    from scratch.
   </para>
   <para>
    On installations where this setting is introduced through an upgrade,
    <literal>trusts_delegated_roles</literal> will be set to
    <literal>heat_stack_owner</literal>. This is a conservative choice to
    prevent breakage in situations where unprivileged users may already have
    been assigned the <literal>heat_stack_owner</literal> role to enable them
    to use Heat but lack the <literal>Member</literal> role. As long as you can
    ensure that all users who have the <literal>heat_stack_owner</literal> role
    also have the <literal>Member</literal> role, it is both safe and
    recommended to change trusts_delegated_roles to <literal>Member</literal>,
    since the latter is the default role assigned by our hybrid LDAP back-end
    among others.
   </para>
   <para>
    To view or change the trusts_delegated_role setting you need to open the
    &o_orch; &barcl; and click <guimenu>Raw</guimenu> in the
    <guimenu>Attributes</guimenu> section. Search for the
    <literal>trusts_delegated_roles</literal> setting and modify the list
    of roles as desired.
   </para>
   <figure>
    <title>the &o_orch; &barcl;: Raw Mode</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_heat_raw.png" width="100%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_heat_raw.png" width="75%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>

   <warning>
    <title>Empty Value</title>
    <para>
     An empty value for <literal>trusts_delegated_roles</literal> will delegate
     <emphasis>all</emphasis> of user roles to Heat. This may create a security
     risk for users who are assigned privileged roles, such as
     <literal>admin</literal>, because these privileged roles will also be
     delegated to the &o_orch; engine when these users create &o_orch; stacks.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.heat.ha">
   <title>&haSetup; for &o_orch;</title>
   <para>
    Making &o_orch; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.ceilometer">
  <title>Deploying &o_meter; (Optional)</title>

  <para>
   <remark condition="clarity">
    2013-10-04 - fs: Which software/billing solution can make use of the
    ceilometer data?
   </remark>
   &o_meter; collects CPU and networking data from &cloud;. This data
   can be used by a billing system to enable customer billing. Deploying
   &o_meter; is optional.
  </para>

  <para>
   For more information about &o_meter; refer to the &ostack;
   documentation at
   <link xlink:href="http://docs.openstack.org/developer/ceilometer/"/>.
  </para>

  <important>
   <title>&o_meter; Restrictions</title>
   <para>
    As of &productname; &productnumber; data measuring is only
    supported for &kvm;, &xen; and Windows &vmguest;s. Other hypervisors and
    &cloud; features such as object or block storage will not be
    measured.
   </para>
  </important>

  <para>
   The following attributes can be configured for &o_meter;:
  </para>

  <variablelist>
   <varlistentry>
    <term>
     Interval used for CPU/disk/network/other meter updates (in seconds)
    </term>
    <listitem>
     <para>
      Specify an interval in seconds after which &o_meter; performs an
      update of the specified meter.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Evaluation interval for threshold alarms (in seconds)</term>
    <listitem>
     <para>
      Set the interval after which to check whether to raise an alarm
      because a threshold has been exceeded. For performance reasons, do not
      set a value lower than the default (60s).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Use MongoDB instead of standard database
    </term>
    <listitem>
     <para>
      &o_meter; collects a large amount of data, which is written to a
      database. In a production system it is recommended to use a separate
      database for &o_meter; rather than the standard database that is
      also used by the other &cloud; components. MongoDB is optimized to
      write a lot of data. As of &productname; &productnumber;, MongoDB
      is only included as a technology preview and not supported.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How long are metering/event samples kept in the database (in days)
    </term>
    <listitem>
     <para>
      Specify how long to keep the data. -1 means that samples are kept in
      the database forever.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Verbose Logging
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_meter; component consists of five different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>ceilometer-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_meter; API server role. This role needs to be deployed on a
      &contrnode;. &o_meter; collects approximately 200 bytes of data
      per hour and &vmguest;. Unless you have a very huge number of
      &vmguest;s, there is no need to install it on a dedicated node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-polling</guimenu></term>
    <listitem>
     <para>
      The polling agent listens to the message bus to collect data. It needs
      to be deployed on a &contrnode;. It can be deployed on the same
      node as <guimenu>ceilometer-server</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-agent</guimenu></term>
    <listitem>
     <para>
      The compute agents collect data from the compute nodes. They need to be
      deployed on all &kvm; and &xen; compute nodes in your cloud (other
      hypervisors are currently not supported).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-swift-proxy-middleware</guimenu></term>
    <listitem>
     <para>
      An agent collecting data from the &swift; nodes. This role needs to
      be deployed on the same node as swift-proxy.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;: Node Deployment</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.ceilometer.ha">
   <title>&haSetup; for &o_meter;</title>
   <para>
    Making &o_meter; highly available requires no special
    configuration&mdash;it is sufficient to deploy the roles
    <guimenu>ceilometer-server</guimenu> and
    <guimenu>ceilometer-polling</guimenu> on a cluster. The cluster needs to
    consist of an odd number of nodes, otherwise the &o_meter; deployment will
    fail.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.depl.ostack.manila">
  <title>Deploying &o_sharefs;</title>
  <para>
   &o_sharefs; provides coordinated access to shared or distributed file
   systems, similar to what &o_blockstore; does for block storage. These file
   systems can be shared between &vmguest;s in &cloud;.
  </para>
  <para>
   &o_sharefs; uses different back-ends. As of &productname; &productnumber;
   currently supported back-ends include <guimenu>Hitachi HNAS</guimenu>, <guimenu>NetApp
   Driver</guimenu>, and <guimenu>CephFS</guimenu>. Two more back-end options, <guimenu>Generic Driver</guimenu> and <guimenu>Other Driver</guimenu> are available for
   testing purposes and are not supported.
  </para>
  <note xml:id="note.limit.cephfs">
   <title>Limitations for CephFS Backend</title>
   <para>
    &o_sharefs; uses some CephFS features that are currently <emphasis>not</emphasis>
    supported by the &sle; 12 SP2 CephFS kernel client:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      RADOS namespaces
     </para>
    </listitem>
    <listitem>
     <para>
      MDS path restrictions
     </para>
    </listitem>
    <listitem>
     <para>
      Quotas
     </para>
    </listitem>
   </itemizedlist>
   <para>
    As a result, to access CephFS shares provisioned by &o_sharefs;, you must
    use ceph-fuse. For details, see <link
    xlink:href="http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html"/>.
    </para>
   </note>
  <para>
   When first opening the &o_sharefs; &barcl;, the default proposal
   <guimenu>Generic Driver</guimenu> is already available for
   configuration. To replace it, first delete it by clicking the trashcan
   icon and then choose a different back-end in the section <guimenu>Add new
   Manila Backend</guimenu>. Select a <guimenu>Type of Share</guimenu>
   and&mdash;optionally&mdash;provide a <guimenu>Name for
   Backend</guimenu>. Activate the back-end with <guimenu>Add
   Backend</guimenu>. Note that at least one back-end must be configured.
  </para>
  <para>
   The attributes that can be set to configure Cinder depend on the back-end:
  </para>

  <bridgehead renderas="sect2"><guimenu>Back-end: Generic</guimenu>
  </bridgehead>

  <para>
   The generic driver is included as a technology preview and is not
   supported.
  </para>

  <bridgehead renderas="sect2"><guimenu>Hitachi HNAS</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>
     <guimenu>Specify which EVS this backend is assigned to</guimenu>
    </term>
    <listitem>
     <para>
      Provide the name of the Enterprise Virtual Server that the selected back-end is assigned to.<remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>Specify IP for mounting shares</guimenu>
    </term>
    <listitem>
     <para>
      IP address for mounting shares.<remark condition="info">2017-01-03 - dpopov: DEVS FIXME More info needed.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>Specify file-system name for creating shares</guimenu>
    </term>
    <listitem>
     <para>
      Provide a file-system name for creating shares.<remark condition="info">2017-01-03 - dpopov: DEVS FIXME More info needed.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>HNAS management interface IP</guimenu>
    </term>
    <listitem>
     <para>
      IP address of the HNAS management interface for communication between Manila controller and HNAS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>HNAS username Base64 String</guimenu>
    </term>
    <listitem>
     <para>
      HNAS username Base64 String required to perform tasks like creating file-systems and network interfaces.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>HNAS user password</guimenu>
    </term>
    <listitem>
     <para>
      HNAS user password. Required only if private key is not provided.<remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>RSA/DSA private key</guimenu>
    </term>
    <listitem>
     <para>
      RSA/DSA private key necessary for connecting to HNAS. Required only if password is not provided.<remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>The time to wait for stalled HNAS jobs before aborting</guimenu>
    </term>
    <listitem>
     <para>
      Time in seconds to wait before aborting stalled HNAS jobs.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: Netapp</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>
     <guimenu>Name of the Virtual Storage Server (vserver)</guimenu>
    </term>
    <listitem>
     <para>
      Host name of the Virtual Storage Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Host Name</guimenu></term>
    <listitem>
     <para>
      The name or IP address for the storage controller or the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Port</guimenu></term>
    <listitem>
     <para>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>User name/Password for Accessing NetApp</guimenu></term>
    <listitem>
     <para>
      Login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu></term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      cluster. Supported protocols are HTTP and HTTPS. Choose the protocol
      your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: CephFS</guimenu>
  </bridgehead>

  <variablelist>
    <varlistentry>
      <term>Use Ceph deployed by Crowbar</term>
      <listitem>
        <para>
          Set to <systemitem>true</systemitem> to use Ceph deployed with Crowbar.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: Manual</guimenu>
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </para>

  <figure>
   <title>The &o_sharefs; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_sharefs; component consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>manila-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_sharefs; server provides the scheduler and the API. Installing it
      on a &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>manila-share</guimenu>
    </term>
    <listitem>
     <para>
      The shared storage service. It can be installed on a &contrnode;, but
      it is recommended to deploy it on one or more dedicated nodes supplied
      with sufficient disk space and networking capacity, since it will
      generate a lot of network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_sharefs; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.manila.ha">
   <title>&haSetup; for &o_sharefs;</title>
   <para>
    While the <guimenu>manila-server</guimenu> role can be deployed on a
    cluster, deploying <guimenu>manila-share</guimenu> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <guimenu>manila-share</guimenu> on several nodes&mdash;this ensures the
    service continues to be available even when a node fails.
   </para>
  </sect2>

 </sect1>

 <sect1 xml:id="sec.depl.ostack.trove">
  <title>Deploying &o_dbaas; (Optional)</title>

  <para>
   &o_dbaas; is a Database-as-a-Service for &cloud;. It provides
   database instances which can be used by all &vmguest;s. With
   &o_dbaas; being deployed, &cloud; users no longer need to deploy
   and maintain their own database applications. For more information about
   &o_dbaas;; refer to the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/trove/"/>.
  </para>

  <important>
   <title>Technology Preview</title>
   <para>
    &o_dbaas; is only included as a technology preview and not supported.
   </para>
  </important>

  <para>
   &o_dbaas; should be deployed on a dedicated &contrnode;.
  </para>

  <para>
   The following attributes can be configured for &o_dbaas;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Enable Trove Volume Support</guimenu>
    </term>
    <listitem>
     <para>
      When enabled, &o_dbaas; will use a &o_blockstore; volume to
      store the data.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Verbose</guimenu>
    </term>
    <listitem>
     <para>
      Increases the amount of information that is written to the log files
      when set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Debug</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dbaas; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_trove.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_trove.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.trove.ha">
   <title>&haSetup; for &o_dbaas;</title>
   <para>
    An &haSetup; for &o_dbaas; is currently not supported.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.tempest">
  <title>Deploying &o_testsuite; (Optional)</title>

  <para>
   &o_testsuite; is an integration test suite for &cloud; written in
   Python. It contains multiple integration tests for validating your &cloud;
   deployment. For more information about &o_testsuite; refer to the &ostack;
   documentation at <link
   xlink:href="http://docs.openstack.org/developer/tempest/"/>.
  </para>

  <important>
   <title>Technology Preview</title>
   <para>
    &o_testsuite; is only included as a technology preview and not supported.
   </para>
   <para>
    &o_testsuite; may be used for testing whether the intended setup will run
    without problems. It should not be used in a production environment.
   </para>
  </important>

  <para>
   &o_testsuite; should be deployed on a &contrnode;.
  </para>

  <para>
   The following attributes can be configured for &o_testsuite;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Choose User name / Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for a regular user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant to be used by &o_testsuite;. If it does not exist, it will be
      created. It is safe to stick with the default value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tempest Admin User name/Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for an admin user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_testsuite; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_tempest.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_tempest.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>


  <tip>
   <title>Running Tests</title>
   <para>
    To run tests with &o_testsuite;, log in to the &contrnode; on which
    &o_testsuite; was deployed. Change into the directory
    <filename>/var/lib/openstack-tempest-test</filename>. To get an overview
    of available commands, run:
   </para>
   <screen>./run_tempest.sh --help</screen>
   <para>
    To serially invoke a subset of all tests (<quote>the gating
    smoketests</quote>) to help validate the working functionality of your
    local cloud instance, run the following command. It will save the output
    to a log file
    <filename>tempest_<replaceable>CURRENT_DATE</replaceable>.log</filename>.
   </para>
   <screen>./run_tempest.sh --no-virtual-env -serial --smoke 2>&amp;1 \
| tee "tempest_$(date +%Y-%m-%d_%H%M%S).log"</screen>
  </tip>

  <sect2 xml:id="sec.depl.ostack.tempest.ha">
   <title>&haSetup; for &o_testsuite;</title>
   <para>
    &o_testsuite; cannot be made highly available.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.depl.ostack.magnum">
  <title>Deploying &o_container; (Optional)</title>
  <para>
   &o_container; is an &ostack; project which offers container orchestration engines for
   deploying and managing containers as first class resources in &ostack;.
  </para>
  <para>
   For more information about &o_container;, see the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/magnum/"/>.
  </para>
  <para>
   For information on how to deploy a Kubernetes cluster (either from command
   line or from the &o_dash; &dash;), see the &cloudsuppl;. It is available from
   <link xlink:href="https://www.suse.com/documentation/cloud"></link>.
  </para>
  <para>
   The following <guimenu>Attributes</guimenu> can be configured for &o_container;:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Logging</guimenu>: <guimenu>Verbose</guimenu></term>
    <listitem>
     <para>
      Increases the amount of information that is written to the log files when
      set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging</guimenu>: <guimenu>Debug</guimenu></term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Trustee Domain</guimenu>: <guimenu>Domain Name</guimenu></term>
    <listitem>
     <para>
      Domain name to use for creating trustee for bays.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Certificate Manager</guimenu>: <guimenu>Plugin</guimenu></term>
    <listitem>
     <para>
      To store certificates, either use the <guimenu>Barbican</guimenu> &ostack;
      service, a local directory (<guimenu>Local</guimenu>), or the <guimenu>Magnum
      Database (x590keypair)</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The &o_container; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_magnum_attributes.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_magnum_attributes.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>The &o_container; &barcl; consists of the following roles:
   <guimenu>magnum-server</guimenu>. It can either be deployed on a &contrnode;
   or on a cluster&mdash;see <xref linkend="sec.depl.ostack.magnum.ha"/>.
   When deploying the role onto a &contrnode;, additional RAM is required for the
   &o_container; server. It is recommended to only deploy the role to a &contrnode;
   that has 16 GB RAM.
  </para>
  <sect2 xml:id="sec.depl.ostack.magnum.ha">
   <title>&haSetup; for &o_container;</title>
   <para>
    Making &o_container; highly available requires no special configuration. It
    is sufficient to deploy it on a cluster.
   </para>
  </sect2>
  </sect1>

  <sect1 xml:id="sec.depl.ostack.barbican">
    <title>Deploying &secret_store; (Optional)</title>
    <para>&secret_store; is a component designed for storing secrets in a secure and standardized manner protected by &o_ident; authentication. Secrets include SSL certificates and passwords used by various &ostack; components.</para>
    <para>&secret_store; settings can be configured in <literal>Raw</literal> mode only. To do this, open the &secret_store; &barcl; <guimenu>Attribute
     </guimenu>configuration in <guimenu>Raw</guimenu> mode.</para>

      <figure>
        <title>The &secret_store; &Barcl;: Raw Mode</title>
        <mediaobject>
          <imageobject role="fo">
       <imagedata fileref="depl_barclamp_barbican_raw.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="depl_barclamp_barbican_raw.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>

    <para>When configuring &secret_store;, pay particular attention to the following settings:</para>
     <itemizedlist>
       <listitem>
         <para><literal>bind_host</literal> Bind host for the &secret_store; API service</para>
       </listitem>
       <listitem>
         <para><literal>bind_port</literal> Bind port for the &secret_store; API service</para>
       </listitem>
       <listitem>
         <para><literal>processes</literal> Number of API processes to run in Apache</para>
       </listitem>
       <listitem>
         <para><literal>ssl</literal> Enable or disable SSL</para>
       </listitem>
       <listitem>
         <para><literal>threads</literal> Number of API worker threads</para>
       </listitem>
       <listitem>
         <para><literal>debug</literal> Enable or disable debug logging</para>
       </listitem>
       <listitem>
         <para><literal>enable_keystone_listener</literal> Enable or disable the &o_ident; listener services</para>
       </listitem>
       <listitem>
         <para><literal>kek</literal> An encryption key (fixed-length 32-byte Base64-encoded value) for &secret_store;'s <systemitem>simple_crypto</systemitem> plugin. If left unspecified, the key will be generated automatically.</para>

         <note>
           <title>Existing Encryption Key</title>
           <para>If you plan to restore and use the existing &secret_store; database after a full reinstall (including a complete wipe of the Crowbar node), make sure to save the specified encryption key beforehand. You will need to provide it after the full reinstall in order to access the data in the restored &secret_store; database.</para>
         </note>
       </listitem>
     </itemizedlist>
<sect2 xml:id="sec.depl.ostack.barbican.ha">
   <title>&haSetup; for &secret_store;</title>
   <para>
    To make &secret_store; highly available, assign the <guimenu>barbican-controller</guimenu> role to the Controller Cluster.
   </para>
  </sect2>
  </sect1>

  <sect1 xml:id="sec.depl.ostack.sahara">
    <title>Deploying &dataproc;</title>
    <para>
      &dataproc; provides users with simple means to provision data processing frameworks (such as Hadoop, Spark, and Storm) on OpenStack. This is accomplished by specifying configuration parameters such as the framework version, cluster topology, node hardware details, etc.
    </para>
    <variablelist>
      <varlistentry>
      <term>Logging: Verbose</term>
      <listitem>
        <para>
          Set to <systemitem>true</systemitem> to increase the amount of information written to the log files.
        </para>
      </listitem>
      </varlistentry>
    </variablelist>

      <figure>
        <title>The &dataproc; Barclamp</title>
        <mediaobject>
          <imageobject role="fo">
       <imagedata fileref="depl_barclamp_sahara.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="depl_barclamp_sahara.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>

<sect2 xml:id="sec.depl.ostack.sahara.ha">
   <title>&haSetup; for &dataproc;</title>
   <para>
     Making &dataproc; highly available requires no special configuration. It is sufficient to deploy it on a cluster.
   </para>
  </sect2>

  </sect1>

 <sect1 xml:id="sec.depl.ostack.final">
  <title>How to Proceed</title>

  <para>
   With a successful deployment of the &ostack; &dash;, the
   &productname; installation is finished. To be able to test your setup
   by starting an &vmguest; one last step remains to be
   done&mdash;uploading an image to the &o_img; component. Refer to the
   &cloudsuppl;, chapter <citetitle>Manage images</citetitle>
<!--<xref linkend="sec.adm.cli.img"/>-->
   for instructions. Images for &cloud; can be built in SUSE Studio.
   Refer to the &cloudsuppl;, section <citetitle>Building Images with
   &susestudio;</citetitle>.
  </para>

  <para>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.&mdash;refer to the &cloudadmin; for details. The
   default credentials for the &ostack; &dash; are user name
   <literal>admin</literal> and password <literal>crowbar</literal>.
  </para>
 </sect1>
</chapter>
