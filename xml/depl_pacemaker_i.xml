<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<sect1 xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="sec.depl.ostack.pacemaker">
 
  <title>Deploying Pacemaker (Optional, &haSetup; Only)</title>

  <para>
   To make the &cloud; controller functions and the &compnode;s highly
   available, set up one or more clusters by deploying Pacemaker (see <xref
   linkend="sec.depl.req.ha"/> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a separate proposal needs to
   be created for each cluster.
  </para>

  <para>
   Deploying Pacemaker is optional. In case you do not want to deploy it,
   skip this section and start the node deployment by deploying the database
   as described in <xref linkend="sec.depl.ostack.db"/>.
  </para>

  <note>
   <title>Number of Cluster Nodes</title>
   <para>
    To set up a cluster, at least two nodes are required. If setting up a
    cluster for storage with replicated storage via DRBD (for example for a cluster for
    the database and RabbitMQ), exactly two nodes are required. For all
    other setups an odd number of nodes with a minimum of three nodes is
    strongly recommended. See <xref linkend="sec.depl.reg.ha.general"/> for
    more information.
   </para>
  </note>

  <para>
   To create a proposal, go to <menuchoice> <guimenu>Barclamps</guimenu>
   <guimenu>OpenStack</guimenu> </menuchoice> and click
   <guimenu>Edit</guimenu> for the Pacemaker &barcl;. A drop-down box
   where you can enter a name and a description for the proposal opens.
   Click <guimenu>Create</guimenu> to open the configuration screen for the
   proposal.
  </para>

  <informalfigure>
   <mediaobject>
     <textobject>
       <phrase>Create Pacemaker Proposal</phrase>
     </textobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <important xml:id="ann.depl.ostack.pacemaker.prop_name">
   <title>Proposal Name</title>
   <para>
    The name you enter for the proposal will be used to generate host names
    for the virtual IPs of HAProxy. By default, the names follow this scheme:
   </para>
   <simplelist>
    <member><literal>cluster-<replaceable>PROPOSAL_NAME</replaceable>.<replaceable>FQDN</replaceable></literal>
    (for the internal name)</member>
   <member><literal>public.cluster-<replaceable>PROPOSAL_NAME</replaceable>.<replaceable>FQDN</replaceable></literal>
    (for the public name)</member>
   </simplelist>
   <para>
   For example, when <replaceable>PROPOSAL_NAME</replaceable> is set to
    <literal>data</literal>, this results in the following names:
   </para>
   <simplelist>
    <member><literal>cluster-data.&exampledomain;</literal>
    </member>
    <member>
     <literal>public.cluster-data.&exampledomain;</literal>
    </member>
   </simplelist>
   <para>
    For requirements regarding SSL encryption and certificates, see <xref
     linkend="sec.depl.req.ssl"/>.
   </para>
  </important>

  <para>
   The following options are configurable in the Pacemaker configuration
   screen:
  </para>

  <variablelist>
   <varlistentry>
    <term>Transport for Communication</term>
    <listitem>
     <para>
      Choose a technology used for cluster communication. You can choose
      between <guimenu>Multicast (UDP)</guimenu>, (sending a message to
      multiple destinations) or <guimenu>Unicast (UDPU)</guimenu> (sending a
      message to a single destination). By default unicast is used.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Policy when cluster does not have quorum</guimenu>
    </term>
    <listitem>
     <para>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <quote>cluster partition</quote> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes
      is defined to have <quote>quorum</quote>.
     </para>
     <para>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See
      <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_global.html"/>,
      section <citetitle>Option no-quorum-policy</citetitle> for details.
     </para>
     <para>
      The recommended setting is to choose <guimenu>Stop</guimenu>. However,
      <guimenu>Ignore</guimenu> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the
      other node fails. For clusters using shared resources, choosing
      <guimenu>freeze</guimenu> may be used to ensure that these resources
      continue to be available.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.pacemaker.barcl.stonith">
    <term>STONITH: Configuration mode for &stonith;
    </term>
    <listitem>
     <para>
      <quote>Misbehaving</quote> nodes in a cluster are shut down to prevent
      it from causing trouble. This mechanism is called &stonith;
      (<quote>Shoot the other node in the head</quote>). &stonith; can be
      configured in a variety of ways, refer to
      <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/cha_ha_fencing.html"/>
      for details. The following configuration options exist:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Configured manually</guimenu>
       </term>
       <listitem>
        <para>
         &stonith; will not be configured when deploying the &barcl;.
         It needs to be configured manually as described in
         <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/cha_ha_fencing.html"/>.
         For experts only.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with IPMI data from the IPMI &barcl;</guimenu>
       </term>
       <listitem>
        <para>
         Using this option automatically sets up &stonith; with data
         received from the IPMI &barcl;. Being able to use this option
         requires that IPMI is configured for all cluster nodes. This should
         be done by default, when deploying cloud. To check or change the
         IPMI deployment, go to <menuchoice> <guimenu>Barclamps</guimenu>
         <guimenu>Crowbar</guimenu> <guimenu>IPMI</guimenu>
         <guimenu>Edit</guimenu> </menuchoice>. Also make sure the
         <guimenu>Enable BMC</guimenu> option is set to
         <guimenu>true</guimenu> on this &barcl;.
        </para>
        <important>
         <title>&stonith; Devices Must Support IPMI</title>
         <para>
          To configure &stonith; with the IPMI data,
          <emphasis>all</emphasis> &stonith; devices must support IPMI.
          Problems with this setup may occur with IPMI implementations that
          are not strictly standards compliant. In this case it is
          recommended to set up &stonith; with &stonith; block devices
          (SBD).
         </para>
        </important>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with &stonith; Block Devices (SBD)</guimenu>
       </term>
       <listitem>
        <para>
         This option requires to manually set up shared storage and a watchdog
         on the cluster nodes before applying the proposal. To do so,
         proceed as follows:
        </para>
        <orderedlist spacing="normal">
         <listitem>
          <para>
           Prepare the shared storage. The path to the shared storage device must
           be persistent and consistent across all nodes in the cluster. The SBD
           device must not use host-based RAID, cLVM2, nor reside on a DRBD*
           instance.
          </para>
         </listitem>
         <listitem>
          <para>
           Install the package <systemitem class="resource">sbd</systemitem>
           on all cluster nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           Initialize SBD device with by running the following command. Make
           sure to replace <filename>/dev/<replaceable>SBD</replaceable></filename> with the path
           to the shared storage device.
          </para>
<screen>sbd -d /dev/<replaceable>SBD</replaceable> create</screen>
          <para>
           Refer to
           <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_storage_protect_fencing.html#pro_ha_storage_protect_sbd_create"/>
           for details.
          </para>
         </listitem>
        </orderedlist>
        <para>In <guimenu>Kernel module for watchdog</guimenu>, specify the
         respective kernel module to be used. Find the most commonly used watchdog
         drivers in the following table:</para>
        <!--taroth 2016-11-28: table taken from
         https://github.com/SUSE/doc-sleha/blob/develop/xml/ha_storage_protection.xml,
         pro.ha.storage.protect.watchdog-->
        <informaltable>
         <tgroup cols="2">
          <thead>
           <row>
            <entry>Hardware</entry>
            <entry>Driver</entry>
           </row>
          </thead>
          <tbody>
           <row>
            <entry>HP</entry>
            <entry><systemitem class="resource">hpwdt</systemitem></entry>
           </row>
           <row>
            <entry>Dell, Fujitsu, Lenovo (Intel TCO)</entry>
            <entry><systemitem class="resource">iTCO_wdt</systemitem></entry>
           </row>
           <row>
            <entry>VM on z/VM on IBM mainframe</entry>
            <entry><systemitem class="resource">vmwatchdog</systemitem></entry>
           </row>
           <row>
            <entry>Xen VM (DomU)</entry>
            <entry><systemitem class="resource">xen_xdt</systemitem></entry>
           </row>
           <row>
            <entry>Generic</entry>
            <entry><systemitem class="resource">softdog</systemitem></entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
        <para>If your hardware is not listed above, either ask your hardware vendor
         for the right name or check the following directory for a list of choices:
         <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/watchdog</filename>.
       </para>
        <para>
         Alternatively, list the drivers that have been installed with your
         kernel version:
        </para>
        <screen>&prompt.root;<command>rpm</command> -ql kernel-<replaceable>VERSION</replaceable> | <command>grep</command> watchdog</screen>
        <para>If the nodes need different watchdog modules, leave the text box
         empty.</para>
        <para>
         After the shared storage has been set up, specify the path using
         the <quote>by-id</quote> notation
         (<filename>/dev/disk/by-id/<replaceable>DEVICE</replaceable></filename>).
         It is possible to specify multiple paths as a comma-separated list.
        </para>
        <para>
         Deploying the &barcl; will automatically complete the SBD setup
         on the cluster nodes by starting the SBD daemon and configuring the
         fencing resource.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>
         Configured with one shared resource for the whole cluster
        </guimenu>
       </term>
       <listitem>
        <para>
         All nodes will use the identical configuration. Specify the
         <guimenu>Fencing Agent</guimenu> to use and enter
         <guimenu>Parameters</guimenu> for the agent.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the
         High Availability Extension, run the following command on an
         already installed cluster nodes: <command>stonith -L</command>. The
         list of parameters depends on the respective agent. To view a list
         of parameters use the following command: </para>
        <screen>stonith -t <replaceable>agent</replaceable> -n</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with one resource per node</guimenu>
       </term>
       <listitem>
        <para>
         All nodes in the cluster use the same <guimenu>Fencing
         Agent</guimenu>, but can be configured with different parameters.
         This setup is, for example, required when nodes are in different
         chassis and therefore need different ILO parameters.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the
         High Availability Extension, run the following command on an
         already installed cluster nodes: <command>stonith -L</command>. The
         list of parameters depends on the respective agent. To view a list
         of parameters use the following command: </para>
        <screen>stonith -t <replaceable>agent</replaceable> -n</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured for nodes running in libvirt</guimenu>
       </term>
       <listitem>
        <para>
         Use this setting for completely virtualized test installations.
         This option is not supported.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="var.depl.ostack.pacemaker.corosync_fencing">
    <term>STONITH: Do not start corosync on boot after fencing</term>
    <listitem>
     <para>
      With &stonith;, Pacemaker clusters with two nodes may sometimes hit
      an issue known as &stonith; deathmatch where each node kills the
      other one, resulting in both nodes rebooting all the time. Another
      similar issue in Pacemaker clusters is the fencing loop, where a
      reboot caused by &stonith; will not be enough to fix a node and it
      will be fenced again and again.
     </para>
     <para>
      This setting can be used to limit these issues. When set to
      <guimenu>true</guimenu>, a node that has not been properly shut down
      or rebooted will not start the services for Pacemaker on boot. Instead,
      the node will wait for action from the &cloud; operator. When set to
      <guimenu>false</guimenu>, the services for Pacemaker will always be
      started on boot. The <guimenu>Automatic</guimenu> value is used to
      have the most appropriate value automatically picked: it will be
      <guimenu>true</guimenu> for two-node clusters (to avoid &stonith;
      deathmatches), and <guimenu>false</guimenu> otherwise.
     </para>
     <para>
      When a node boots but not starts corosync because of this setting,
      then the node's status is in the <guimenu>Node
      Dashboard</guimenu> is set to "<literal>Problem</literal>" (red
      dot). To make this node usable again, see
      <xref linkend="sec.deploy.ha_recovery.contr.node.add"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Mail Notifications: Enable Mail Notifications</term>
    <listitem>
     <para>
      Get notified of cluster node failures via e-mail. If set to
      <guimenu>true</guimenu>, you need to specify which <guimenu>SMTP
      Server</guimenu> to use, a prefix for the mails' subject and sender
      and recipient addresses. Note that the SMTP server must be accessible
      by the cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>DRBD: Prepare Cluster for DRBD</term>
    <listitem>
     <para>
      Set up DRBD for replicated storage on the cluster. This option
      requires a two-node cluster with a spare hard disk for each node. The
      disks should have a minimum size of 100 GB. Using DRBD is recommended
      for making the database and RabbitMQ highly available. For other
      clusters, set this option to <guimenu>False</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HAProxy: Public name for public virtual IP</guimenu>
    </term>
    <listitem>
     <para>
      The public name is the host name that will be used instead of the
      generated public name (see
      <xref linkend="ann.depl.ostack.pacemaker.prop_name"/>) for the public
      virtual IP of &haproxy;. (This is the case when registering public endpoints, for
      example). Any name specified here needs to be resolved by a name
      server placed outside of the &cloud; network.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The Pacemaker component consists of the following roles. Deploying the
   <guimenu>hawk-server</guimenu> role is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>pacemaker-cluster-member</guimenu>
    </term>
    <listitem>
     <para>
      Deploy this role on all nodes that should become member of the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>hawk-server</guimenu>
    </term>
    <listitem>
     <para>
      Deploying this role is optional. If deployed, sets up the &hawk;
      Web interface which lets you monitor the status of the cluster. The
      Web interface can be accessed via
      <literal>https://<replaceable>IP-ADDRESS</replaceable>:7630</literal>.
      Note that the GUI on &cloud; can only be used to monitor the
      cluster status and not to change its configuration.
     </para>
     <para>
      <guimenu>hawk-server</guimenu> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>pacemaker-remote</guimenu></term>
    <listitem>
     <para>Deploy this role on all nodes that should become members of the
     &compnode;s cluster. They will run as Pacemaker remote nodes that are
     controlled by the cluster, but do not affect quorum. Instead of the
     complete cluster stack, only the <literal>pacemaker-remote</literal>
     component will be installed on this nodes.
      </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   After a cluster has been successfully deployed, it is listed under
   <guimenu>Available Clusters</guimenu> in the
   <guimenu>Deployment</guimenu> section and can be used for role deployment
   like a regular node.
  </para>

  <warning>
   <title>Deploying Roles on Single Cluster Nodes</title>
   <para>
    When using clusters, roles from other &barcl;s must never be deployed
    to single nodes that are already part of a cluster. The only exceptions
    from this rule are the following roles:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      cinder-volume
     </para>
    </listitem>
    <listitem>
     <para>
      swift-proxy + swift-dispersion
     </para>
    </listitem>
    <listitem>
     <para>
      swift-ring-compute
     </para>
    </listitem>
    <listitem>
     <para>
      swift-storage
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <!--taroth 2017-02-01: commenting as it is still not possible to update this screenshot-->
  <!-- <figure>
   <title>Available Clusters in the Deployment Section</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>-->
  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never</emphasis> manually start
    or stop an HA-managed service (or configure it to start on boot). Services
    may only be started or stopped by using the cluster management tools Hawk
    or the crm shell. See
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_resources.html"/>
    for more information.
   </para>
  </important>
  <note>
   <title>Testing the Cluster Setup</title>
   <para>
    To check whether all cluster resources are running, either use the
    &hawk; Web interface or run the command <command>crm_mon</command>
    <option>-1r</option>. If it is not the case, clean up the respective
    resource with <command>crm</command> <option>resource</option>
    <option>cleanup</option> <replaceable>RESOURCE</replaceable> , so it
    gets respawned.
   </para>
   <para>
    Also make sure that &stonith; correctly works before continuing with
    the &cloud; setup. This is especially important when having chosen a
    &stonith; configuration requiring manual setup. To test if
    &stonith; works, log in to a node on the cluster and run the
    following command:
   </para>
<screen>pkill -9 corosync</screen>
   <para>
    In case &stonith; is correctly configured, the node will reboot.
   </para>
   <para>
    Before testing on a production cluster, plan a maintenance window in
    case issues should arise.
   </para>
  </note>
 </sect1>
